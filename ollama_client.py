import requests

OLLAMA_URL = "http://localhost:11434"

def chat_with_ollama(prompt: str, model: str = "llama3.2:3b"):
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": "ë„ˆëŠ” ë²•ë¥ Â·ì»´í”Œë¼ì´ì–¸ìŠ¤ ë¶„ì„ AIë‹¤."},
            {"role": "user", "content": prompt}
        ],
        "stream": False,
        "options": {
            "num_predict": 16   # ğŸ”¥ í•µì‹¬: ì§§ê²Œ ìƒì„±
        }
    }

    response = requests.post(
        f"{OLLAMA_URL}/api/chat",
        json=payload,
        timeout=300  # ğŸ”¥ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œë§Œ ëŠ˜ë¦¼
    )

    response.raise_for_status()
    return response.json()["message"]["content"]